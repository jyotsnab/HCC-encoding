{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Preprocessing for High Cardinality Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Emperical Bayes method\n",
    "\n",
    "> #### a) Classification Problems:\n",
    "\n",
    ">For binary classification problems $Y\\in$ [0,1] , the categorical variable X is mapped to a scalar value $S_i$ representing an estimate of the probability of $Y=1$ for each unique categorical variable $X = Xi$ \n",
    "> \n",
    "> Basic principle $\\to$  $ S_i \\cong P(Y | X=Xi )$\n",
    "> $$ S_i = \\lambda (n_i) \\frac {( n_{iY}  )}{n_i} + (1- \\lambda (n_i)) \\frac {n_Y}{n_{TR}}$$ \n",
    "\n",
    "\n",
    "> where, \n",
    "\n",
    "> $ n_y \\to $ total number of cases such that $ Y= 1$\n",
    "\n",
    "> $n_i \\to $  total number of cases of $ X= X_i $\n",
    "\n",
    "> $n_{iY} \\to $ number of cases of $Y = 1 |$  $X= X_i$ \n",
    "\n",
    "> $n_{TR} \\to $ total number of cases/samples in the dataset\n",
    "\n",
    "> $ \\lambda (n_i) \\to $ weighing factor, $ 0 \\le \\lambda (n_i) \\le 1$\n",
    "\n",
    ">> $ \\lambda $ is a monotonically increasing function. There are many forms of this function. This function helps in assigning weightage to posterior probability and prior probability in calculating $ S_i $ based on the number of cases for each $ Xi $. The rationale is to assign $S_i$ to prior probability of $Y$ in the case of small $n_i$ and give more weightage to posterior probability for large values of $ n_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> #### b) Regression Problems:\n",
    "\n",
    "\n",
    "> For regression tasks the categorical values are mapped to a scalar value $S_i$ to $ E[Y] $ at $ X= X_i$\n",
    "\n",
    "> $$ S_i = \\lambda (n_i) \\frac { \\sum_{k \\in L_i} Y_k}{n_i} + ( 1- \\lambda (n_i)) \\frac { \\sum_{k=1}^{N_{TR}} Y_k }{n_{TR}}$$\n",
    "\n",
    "\n",
    "> where,\n",
    "\n",
    "> $ L_i \\to $ is the set of observations of size $n_i$ for which $X=X_i$\n",
    ">\n",
    ">\n",
    "> #### c) Categorical variables with hierarchical structure\n",
    ">  \n",
    "> for variables like address: state $\\to$ city $\\to$ street, the above formulae can be replaced with:\n",
    "\n",
    "> $$ S_i^{street} = \\lambda (n_i) \\frac {( n_{iY}  )}{n_i} + (1- \\lambda (n_i)) S_i^{city} $$\n",
    "\n",
    "\n",
    "*NOTE : for cases of missing categorical values, this algorithm treates these as a separate 'nan' variable and performs the same encoding as other variables.*\n",
    "\n",
    "*** Reference : Daniele Micci-Barreca. 2001. A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems. SIGKDD Explor. Newsl. 3, 1 (July 2001), 27-32 ***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Supervised Ratio\n",
    "\n",
    "\n",
    ">  For a classification task, the scalar value for a categorical value is replaced with a fraction of positive $Y$  values for the respective $X_i$ of category variable :\n",
    "\n",
    ">  $$ S_i = \\frac{n_{iY}}{n_i} $$\n",
    "\n",
    ">*NOTE: This method has been used to transform ZIP / PIN code variables.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Weight of Evidence\n",
    "\n",
    "\n",
    ">  In this method, any mapped categorical variable with less than average $Y$ value with have a negative scalar value and a higher than average value of $Y$ will lead to a positive value.\n",
    "\n",
    ">  This method is only defined for classification problems.\n",
    "\n",
    ">  $$ S_i = ln( \\frac { n_{iY} / n_Y}{(n_i - n_{iY}) / (n_{TR} - n_Y )}  )$$\n",
    "\n",
    ">*Notes: This type of transformation have been found useful for problems like churn rate problems and has been found to perform better compared to method of 'Supervised Ratio' described earlier.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4. Perlich ratio\n",
    "\n",
    "> This method buils on weight of evidence method and is also only defined for classification tasks.\n",
    "\n",
    ">  Perlich ratio is given by:\n",
    "\n",
    ">  $$ PR = \\frac {P_1}{P_1 + P_2} $$\n",
    "\n",
    "> where,\n",
    "\n",
    "> $$ P_1 = \\frac {n_{iY}}{\\alpha}$$\n",
    "\n",
    ">> $ P_1 $ is the cosine distance between case vector and the reference vector, given by: \n",
    "\n",
    ">> $$ \\alpha = \\sqrt {(n_{1Y})^2+ (n_{2Y})^2+(n_{3Y})^2+ ..... + (n_{mY})^2}$$ for m unique categorical values.\n",
    "\n",
    "> $$ P_2 = \\frac {n_i- n_{iY}}{\\beta}$$\n",
    "\n",
    ">where,\n",
    "\n",
    ">> $ P_2 $ is the cosine distance between case vector and the reference vector, given by: \n",
    "\n",
    ">> $$ \\beta = \\sqrt {(n_1 - n_{1Y})^2+ (n_2 - n_{2Y})^2+(n_3 - n_{3Y})^2+ ..... + (n_m - n_{mY})^2}$$ for m unique categorical values.\n",
    "\n",
    ">> For unseen values, the catogorical variable can be replaced with: \n",
    "\n",
    ">> $$ PR =  \\frac {\\alpha}{\\alpha + \\beta}$$ \n",
    "\n",
    ">> i.e. $ P_1 = P_2 \\to $the cosine distance between positive and negative refence vectors is consired to be same\n",
    "\n",
    "*** Reference (for 2,3,4): Moeyersoms, J., & Martens, D. (2015). Including high-cardinality attributes in predictive models: A case study inchurn prediction in the energy sector. Decision Support Systems, 72, 72â€“81\n",
    "[view paper](https://www.researchgate.net/publication/272522918_Including_High-Cardinality_Attributes_in_Predictive_Models_a_Case_Study_in_Churn_Prediction_in_the_Energy_Sector) ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5. K-modes\n",
    "\n",
    "> K-modes is used for clustering using categorical variables based on number of matching categories between data points.\n",
    "\n",
    "> you can get more details in this [paper](https://grid.cs.gsu.edu/~wkim/index_files/papers/kprototype.pdf).\n",
    "\n",
    "\n",
    "### 6. K-prototypes\n",
    "\n",
    "> K- prototypes combines K-means and k-modes and so, it can cluster mixed data types.\n",
    "\n",
    "> you can get more details in this [paper](http://scit.sxu.edu.cn/Paper/2012052223413604.pdf) \n",
    "\n",
    "### 7. LEAVE-ONE-OUT encoding\n",
    "\n",
    ">  It proposes to maps $X_i$ to a scalar value $S_i$ using the following formula:\n",
    "\n",
    ">  $$ S_i = \\frac {E[Y_i] - y_i }{n_i - 1} $$ \n",
    "\n",
    "\n",
    "> *Note : This is not based an approach described by a Kaggle winner. No other academic records justifying this approach were found.*\n",
    "\n",
    "> *Reference: : https://github.com/owenzhang/kaggle-avazu/blob/master/utils.py *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 8. Entity Embedding with Neural Networks\n",
    "\n",
    ">  goal of Neural Network is to learn a function: $ y = f(x1,x2,x3,...,x_n) $\n",
    "\n",
    ">  each discrete variable is to be mapped to a vector: $ e_i: x_i \\to $ xi\n",
    "\n",
    "\n",
    "\n",
    "<img src= "../entity_embedding.png">\n",
    "\n",
    ">  result of a NN embedding :    $ Xi \\to w_{xi\\beta}$\n",
    "\n",
    "\n",
    "\n",
    "> *Reference: : https://arxiv.org/pdf/1604.06737.pdf *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
